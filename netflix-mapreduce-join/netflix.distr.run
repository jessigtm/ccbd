#!/bin/bash
#SBATCH --job-name="netflix"
#SBATCH --output="netflix.distr.out"
#SBATCH --partition=compute
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=128
#SBATCH --mem=249208M
#SBATCH --time=29

# Load Required Modules
module load cpu/0.15.4 gcc/7.5.0 openjdk

#  Configure Hadoop & MyHadoop 
export HADOOP_HOME=/path/to/hadoop
export MYHADOOP_HOME=/path/to/myhadoop
export HADOOP_CONF_DIR=/path/to/hadoop/conf
export PATH="$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$MYHADOOP_HOME/bin:$PATH"

# Set Up Distributed Cluster 
myhadoop-configure.sh -s /scratch/$USER/job_$SLURM_JOBID -i "s/$/.ib.cluster/"
cp $HADOOP_CONF_DIR/slaves $HADOOP_CONF_DIR/workers

#  Start HDFS and YARN 
start-dfs.sh
start-yarn.sh

#  Prepare Input Data 
hdfs dfs -mkdir -p /user/$USER/
hdfs dfs -put /path/to/ratings.txt /user/$USER/
hdfs dfs -put /path/to/movie_titles.csv /user/$USER/

#  Run MapReduce Join Job 
hadoop jar netflix.jar Netflix /user/$USER/ratings.txt /user/$USER/movie_titles.csv /user/$USER/output

#  Retrieve Output 
rm -f large-output.txt
hdfs dfs -getmerge /user/$USER/output large-output.txt

#  Stop Services & Cleanup 
stop-yarn.sh
stop-dfs.sh
myhadoop-cleanup.sh
